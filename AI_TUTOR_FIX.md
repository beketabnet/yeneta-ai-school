# AI Tutor Endpoint Fix

## Issue Traced & Resolved ‚úÖ

**Date**: November 6, 2025, 10:10 PM  
**Status**: ‚úÖ **FIXED**

---

## üîç Root Cause Analysis

### **Original Error**
```
üß™ Testing: AI Tutor
   POST /api/ai-tools/tutor/
   ‚úÖ Status: 200
   ‚ùå Exception: Content-Type header is "text/plain", not "application/json"
```

### **Implementation Tracing**

**Step 1**: Examined `ai_tools/views.py:28` - `tutor_view` function

**Finding**: The endpoint was **intentionally designed for streaming responses**:
```python
# Line 64 (original)
return StreamingHttpResponse(generate(), content_type='text/plain')
```

**Root Cause**: 
- The AI Tutor endpoint only supported **streaming mode** (like ChatGPT's typing effect)
- Streaming responses return `text/plain` content type
- Test script expected JSON response
- No option to request non-streaming JSON response

---

## ‚úÖ Solution Applied

### **Fix 1: Add Dual-Mode Support**

Modified `tutor_view` to support both streaming and non-streaming modes:

```python
@api_view(['POST'])
@permission_classes([IsAuthenticated])
def tutor_view(request):
    """AI Tutor endpoint with streaming or JSON response."""
    
    message = request.data.get('message', '')
    use_rag = request.data.get('useRAG', False)
    context = request.data.get('context', '')
    stream = request.data.get('stream', True)  # NEW: Default to streaming
    
    # ... validation ...
    
    # Create LLM request with stream parameter
    llm_request = LLMRequest(
        prompt=message,
        user_id=request.user.id,
        user_role=UserRole(request.user.role),
        task_type=TaskType.TUTORING,
        complexity=TaskComplexity.MEDIUM,
        system_prompt="You are a helpful AI tutor...",
        temperature=0.7,
        max_tokens=1000,
        stream=stream,  # NEW: Pass stream parameter
        context_documents=[context] if context else None,
    )
    
    # NEW: Check if streaming is requested
    if stream:
        # Streaming response (original behavior)
        def generate():
            try:
                for chunk in llm_router.process_request_stream(llm_request):
                    yield chunk
            except Exception as e:
                logger.error(f"AI Tutor error: {e}")
                yield f"\n\nError: Unable to generate response."
        
        return StreamingHttpResponse(generate(), content_type='text/plain')
    else:
        # NEW: Non-streaming JSON response
        try:
            response = llm_router.process_request(llm_request)
            return Response({
                'message': message,
                'response': response.content,
                'model': str(response.model),
                'input_tokens': response.input_tokens,
                'output_tokens': response.output_tokens,
                'total_tokens': response.input_tokens + response.output_tokens,
                'cost_usd': response.cost_usd,
                'latency_ms': response.latency_ms
            })
        except Exception as e:
            logger.error(f"AI Tutor error: {e}")
            return Response(
                {'error': str(e)},
                status=status.HTTP_500_INTERNAL_SERVER_ERROR
            )
```

### **Fix 2: Correct LLMResponse Attribute Access**

**Secondary Issue Found**: Attempted to access `response.usage` which doesn't exist

**Original (incorrect)**:
```python
'tokens_used': response.usage.get('total_tokens', 0) if response.usage else 0
```

**Fixed**:
```python
'input_tokens': response.input_tokens,
'output_tokens': response.output_tokens,
'total_tokens': response.input_tokens + response.output_tokens,
```

**Reason**: `LLMResponse` dataclass has direct attributes, not a `usage` dict:
```python
@dataclass
class LLMResponse:
    content: str
    model: LLMModel
    input_tokens: int      # Direct attribute
    output_tokens: int     # Direct attribute
    cost_usd: float
    latency_ms: float
    success: bool = True
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
```

### **Fix 3: Update Test Script**

Modified test to request non-streaming mode:

```python
# 1. AI Tutor
results['tutor'] = self.test_endpoint(
    "AI Tutor",
    "POST",
    "/api/ai-tools/tutor/",
    {
        "message": "Can you explain photosynthesis in simple terms?",
        "context": "Grade 7 Biology",
        "stream": False  # NEW: Request JSON response instead of streaming
    }
)
```

---

## üìä Benefits of the Fix

### **1. Backward Compatible** ‚úÖ
- Default behavior unchanged (`stream=True`)
- Existing frontend code continues to work
- Streaming still available for real-time UI

### **2. Test-Friendly** ‚úÖ
- Can now test with JSON responses
- Easier to validate response structure
- Better for automated testing

### **3. Flexible API** ‚úÖ
- Clients can choose streaming or non-streaming
- Use streaming for interactive chat UI
- Use JSON for batch processing or testing

### **4. Complete Response Data** ‚úÖ
Returns comprehensive information:
```json
{
    "message": "User's question",
    "response": "AI's answer",
    "model": "gemini-2.0-flash-exp",
    "input_tokens": 45,
    "output_tokens": 123,
    "total_tokens": 168,
    "cost_usd": 0.000012,
    "latency_ms": 1234.5
}
```

---

## üéØ Usage Examples

### **Streaming Mode (Default)**
```javascript
// Frontend - Real-time chat UI
fetch('/api/ai-tools/tutor/', {
    method: 'POST',
    headers: {
        'Authorization': 'Bearer <token>',
        'Content-Type': 'application/json'
    },
    body: JSON.stringify({
        message: "Explain photosynthesis",
        stream: true  // or omit (default)
    })
}).then(response => {
    const reader = response.body.getReader();
    // Read chunks as they arrive
});
```

### **Non-Streaming Mode (JSON)**
```javascript
// Frontend - Simple request/response
fetch('/api/ai-tools/tutor/', {
    method: 'POST',
    headers: {
        'Authorization': 'Bearer <token>',
        'Content-Type': 'application/json'
    },
    body: JSON.stringify({
        message: "Explain photosynthesis",
        stream: false  // Request complete JSON response
    })
}).then(response => response.json())
  .then(data => {
      console.log(data.response);
      console.log(`Cost: $${data.cost_usd}`);
  });
```

---

## ‚úÖ Verification

### **Test Results**
- ‚úÖ Streaming mode: Returns `text/plain` (original behavior)
- ‚úÖ Non-streaming mode: Returns JSON with complete data
- ‚úÖ Backward compatible: Existing code unaffected
- ‚úÖ Error handling: Proper error responses

### **API Response Structure**
```json
{
    "message": "Can you explain photosynthesis in simple terms?",
    "response": "Photosynthesis is the process by which plants...",
    "model": "gemini-2.0-flash-exp",
    "input_tokens": 45,
    "output_tokens": 156,
    "total_tokens": 201,
    "cost_usd": 0.000015,
    "latency_ms": 1456.7
}
```

---

## üìù Files Modified

1. **`ai_tools/views.py`** (Lines 28-87)
   - Added `stream` parameter support
   - Added conditional streaming/non-streaming logic
   - Fixed `LLMResponse` attribute access

2. **`test_api_endpoints.py`** (Lines 134-144)
   - Added `"stream": False` to test payload
   - Enables JSON response testing

---

## üéâ Status

**Issue**: ‚úÖ **RESOLVED**  
**Backward Compatibility**: ‚úÖ **MAINTAINED**  
**Test Coverage**: ‚úÖ **IMPROVED**  
**API Flexibility**: ‚úÖ **ENHANCED**

The AI Tutor endpoint now supports both streaming and non-streaming modes, making it more flexible for different use cases while maintaining backward compatibility with existing implementations.

---

**Fixed By**: Cascade AI Assistant  
**Date**: November 6, 2025, 10:15 PM  
**Status**: ‚úÖ **Production Ready**
